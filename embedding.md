- [Embedding](#embedding)
  - [Methods](#methods)
    - [`embed_documents()`](#embed_documents)
    - [`embed_query()`](#embed_query)
  - [Summary](#summary)
- [Vector Store](#vector-store)
  - [Why use vector store?](#why-use-vector-store)
  - [Methods](#methods-1)
    - [FAISS](#faiss)
      - [FAISS in LangChain](#faiss-in-langchain)
      - [Features](#features-of-faiss)
      - [Example](#example)
      - [Saving](#saving)
      - [FAISS Index Types](#faiss-index-types-advanced)
    - [Chroma](#chroma)
      - [Chroma in LangChain](#chroma-in-langchain)
      - [Features](#features-of-chroma)
      - [Example](#example-1)
      - [Saving](#saving-1)
      - [When to Use Chroma?](#when-to-use-chroma)

# Embedding

An embedding is a fixed-size vector representation of text (words, sentences, or documents). It captures semantic meaning—so that similar texts result in similar vectors.

It is used to convert text into numerical vectors. These vectors can then be used for semantic search, similarity comparison, retrieval-augmented generation (RAG), and context-aware applications.

Example:

- “How are you?” and “How’s it going?” → Similar embeddings.
- “How are you?” and “What is the capital of France?” → Dissimilar embeddings.

LangChain integrates with several embedding models, such as:

- OpenAI (`OpenAIEmbeddings`)
- HuggingFace models (`HuggingFaceEmbeddings`)
- Cohere, Google’s Palm, etc.

Embedding uses:

- Vector stores (e.g., FAISS, Chroma, Pinecone)
- Document search and retrieval
- Memory for agents
- Similarity comparison

## Methods

### `embed_documents()`

- Converts a list of documents (e.g., pages, sentences) into vector embeddings.
- These embeddings are stored in a vector store (like FAISS, Chroma) to enable similarity search later

**Input:**

```
["Text from doc 1", "Another text", "Third doc text"]
```

**Output:**

```
[
  [0.123, 0.456, ...],   # vector for doc 1
  [0.321, 0.654, ...],   # vector for doc 2
  ...
]
```

**Example:**

```py
texts = ["Pandas is great for data analysis.", "Paris is the capital of France."]
vectors = embedding_model.embed_documents(texts)

for i, vec in enumerate(vectors):
    print(f"Document {i+1} Vector (length {len(vec)}): {vec[:10]}")
```

`CharacterTextSplitter` also create a chunk of `Document` object, but it contains text data instead of vector(numeric) data.

### `embed_query()`

- Converts a single search query into a vector.
- This vector is compared to stored document vectors using cosine similarity or similar metrics.
  **Input:**

```
"What is the capital of France?"
```

**Output:**

```
[0.234, 0.598, 0.983, ...]  # Single vector
```

**Example:**

```py
query = "Which country has Paris as its capital?"
query_vector = embedding_model .embed_query(query)
```

You then compare `query_vector` to document vectors to find the most semantically similar documents.

### Summary

| Method              | Purpose                                | Input Type  | Output Type         | Use Case                        |
| ------------------- | -------------------------------------- | ----------- | ------------------- | ------------------------------- |
| `embed_documents()` | Embed **many texts (e.g., documents)** | `List[str]` | `List[List[float]]` | Storing in a vector database    |
| `embed_query()`     | Embed **a single query string**        | `str`       | `List[float]`       | Comparing with document vectors |

Embedding models like `sentence-transformers` often tune representations differently:

- `embed_documents()` produces contextualized representations optimized for retrieval.
- `embed_query()` may apply query-specific preprocessing to match better with stored embeddings.

This distinction is especially relevant in bi-encoder models (common in semantic search), where query and document are encoded separately.

These are used together like this:

```py
# Search using the query
results = vectorstore.similarity_search_by_vector(query_vector)
# vectorStore is created for storing embeded document, follow vector store to see details
```

# Vector Store

A Vector Store in LangChain is a component used to store and retrieve vector representations (embeddings) of text data. These embeddings capture the semantic meaning of the text, allowing for efficient similarity search. Vector stores are crucial in building retrieval-based applications like question answering, chatbots, or semantic search engines.

A Vector Store holds:

1. Text data (like documents or chunks of documents).
2. Embeddings of the text – high-dimensional numerical representations generated by an embedding model (e.g., OpenAI, HuggingFace).
3. An index to perform fast similarity searches based on a query vector.

LangChain integrates with many popular vector store backends:

- FAISS
- Chroma
- Pinecone
- Weaviate
- Qdrant
- Milvus

## Why use vector store?

Suppose you're building a QA system over your PDF notes. You:

1. Split the notes into chunks.
2. Embed each chunk.
3. Store these embeddings in a vector store.
4. When a question is asked, you embed the query, and use similarity search in the vector store to find the most relevant chunks.

## Methods

### FAISS

FAISS (Facebook AI Similarity Search) is an open-source library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It's widely used for building fast, scalable, and memory-efficient vector databases, particularly for applications like:

- Semantic search
- Recommendation systems
- Question answering (RAG)
- Document similarity

#### FAISS in LangChain

LangChain wraps FAISS with an easy-to-use interface that:

- Accepts a list of documents or text chunks.
- Embeds them using an embedding model (e.g., OpenAI, Hugging Face).
- Stores the vectors in memory or on disk.
- Provides functions to search for the most semantically similar chunks given a query.

LangChain abstracts the FAISS indexing and retrieval mechanisms so you can use it with minimal setup.

#### Features of FAISS

| Feature                                      | Description                                                          |
| -------------------------------------------- | -------------------------------------------------------------------- |
| **In-memory** or **disk-persisted** indexing | You can keep it in RAM or save/load to disk.                         |
| Fast similarity search                       | Optimized for cosine, L2 distances.                                  |
| Pluggable embeddings                         | Works with any embedding model (OpenAI, HuggingFace, etc.).          |
| Works with `Document` objects                | Integrates with LangChain’s `Document` format for metadata handling. |

#### Example

```py
vectorstore = FAISS.from_documents(documents, embedding_model)
```

`FAISS` doesn't require embeded document, chunk text(splitted text) require.

- `doucments` - Chunk document splitted by `CharacterTextSplitter`.
- `embedding_model` - Embedding model.

**Extended Example:**

```py
docs = [
    Document(
        metadata={'source': 'T:/project/programming_notes/ai/pandas.md'},
        page_content='Pandas is used for working with data sets, it is used to analyze data. It has functions for analyzing, cleaning, exploring, and manipulating data.'),
    Document(
        metadata={'source': 'T:/project/programming_notes/ai/pandas.md'},
        page_content='__What Can Pandas Do?__\nPandas gives you answers about the data. Like:\n- Is there a correlation between two or more columns?\n- What is average value?\n- Max value?\n- Min value?')
    ]
embedding_model = OpenAIEmbeddings()  # Requires your OpenAI API key
vectorstore = FAISS.from_documents(documents, embedding_model)

# Query the vector store
query = "How do you build a chatbot with LangChain?"

# Retrieve top 2 most similar documents
results = vectorstore.similarity_search(query, k=2)

# Print the results
for i, res in enumerate(results):
    print(f"\nResult {i+1}:")
    print("Text:", res.page_content)
    print("Metadata:", res.metadata)
```

#### Saving

```py
# Save index
vectorstore.save_local("my_faiss_index")

# Load index
loaded_vectorstore = FAISS.load_local("my_faiss_index", embedding_model)
```

#### FAISS Index Types (Advanced)

FAISS supports various index types:

- ``IndexFlatL2` – Simple, exact search.
- ``IndexIVFFlat` – Approximate, faster but requires training.
- ``IndexHNSWFlat` – Graph-based for high speed.

LangChain defaults to `IndexFlatL2`, but you can customize it for large-scale datasets.

### Chroma

Chroma is an open-source embedding database designed for:

- Storing documents + embeddings
- Performing fast similarity search
- Serving as a local vector store backend

It’s commonly used with LangChain to build retrieval-based applications like:

- Document Q&A
- Chatbots with memory
- Semantic search

#### Chroma in LangChain

Chroma is especially useful when you want:

- Quick local storage and retrieval of embeddings (no cloud needed)
- Persistence (you can save and reload the database)
- Metadata handling
- Tight integration with LangChain

#### Features of Chroma

| Feature                                       | Description                             |
| --------------------------------------------- | --------------------------------------- |
| **Local**                                     | Runs in-memory or persists to disk      |
| **Fast**                                      | Built for real-time embedding retrieval |
| **Simple API**                                | Minimal configuration needed            |
| **Supports metadata**                         | Store and filter on metadata fields     |
| **Compatible with multiple embedding models** | OpenAI, Hugging Face, etc.              |

#### Example

```py
# Create Chroma vector store (optionally with persistence)
vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory="chroma_db")
```

#### Saving

Chroma persists automatically if you set `persist_directory`.

```py
# Save Chroma index
vectorstore.persist()

# Load later
vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embedding_model)
```

#### When to Use Chroma?

| Use Case                     | Why Chroma?                                     |
| ---------------------------- | ----------------------------------------------- |
| **Local dev or prototyping** | Lightweight, fast, no cloud setup               |
| **QA over documents**        | Embedding + retrieval made simple               |
| **Persistent vector DB**     | Just set `persist_directory`                    |
| **Agent-based tools**        | Can be integrated as a LangChain retriever tool |
